# Complete System Architecture

## ğŸš€ Complete Feature Set Implementation

Your ASR system now includes all features from the specification:

## ğŸ“Š Complete System Architecture

```
Vietnamese ASR System (Complete)

â”œâ”€â”€ ğŸ¯ Core ASR Models
â”‚   â”œâ”€â”€ Basic ASR (LSTM + CTC)          âœ… models/lstm_asr.py
â”‚   â””â”€â”€ Enhanced ASR (Transformer + Contextual Embeddings)  âœ… models/enhanced_asr.py
â”‚
â”œâ”€â”€ ğŸ§  Embedding Systems
â”‚   â”œâ”€â”€ Our Enhanced Embeddings:
â”‚   â”‚   â”œâ”€â”€ Subword tokenization (BPE) âœ… preprocessing/bpe_tokenizer.py
â”‚   â”‚   â”œâ”€â”€ Contextual embeddings (Transformer)  âœ… models/enhanced_asr.py
â”‚   â”‚   â”œâ”€â”€ Cross-modal attention       âœ… models/enhanced_asr.py
â”‚   â”‚   â””â”€â”€ Word2Vec auxiliary training âœ… models/enhanced_asr.py
â”‚   â”‚
â”‚   â””â”€â”€ AI2text Embeddings Patch:
â”‚       â”œâ”€â”€ Semantic Word2Vec (transcript-based)  âœ… nlp/word2vec_trainer.py
â”‚       â”œâ”€â”€ Phon2Vec (phonetic/sound embeddings) âœ… nlp/phon2vec_trainer.py
â”‚       â”œâ”€â”€ FAISS indexing              âœ… (from embeddings patch)
â”‚       â””â”€â”€ N-best rescoring            âœ… decoding/rescoring.py
â”‚
â”œâ”€â”€ ğŸ”„ Training Pipeline
â”‚   â”œâ”€â”€ Multi-task learning (CTC + Word2Vec)  âœ… models/enhanced_asr.py
â”‚   â”œâ”€â”€ Mixed precision training              âœ… training/train.py
â”‚   â”œâ”€â”€ Callbacks (Checkpoint, EarlyStop, Logging)  âœ… training/callbacks.py
â”‚   â””â”€â”€ Database experiment tracking          âœ… database/db_utils.py
â”‚
â””â”€â”€ ğŸ¯ Evaluation & Inference
    â”œâ”€â”€ Beam search decoding           âœ… decoding/beam_search.py
    â”œâ”€â”€ N-best rescoring with embeddings âœ… decoding/rescoring.py
    â”œâ”€â”€ Contextual biasing              âœ… decoding/rescoring.py
    â””â”€â”€ WER/CER metrics                 âœ… utils/metrics.py
```

## âœ… Feature Implementation Status

### Core ASR System âœ…

1. **LSTM + CTC Model** (`models/lstm_asr.py`)
   - Bidirectional LSTM layers
   - Convolutional feature extraction
   - CTC decoder
   - Suitable for weak hardware

2. **Transformer + CTC Model** (`models/asr_base.py`)
   - Transformer encoder with attention
   - Convolutional subsampling
   - CTC decoder
   - Production-ready architecture

3. **Enhanced Transformer** (`models/enhanced_asr.py`)
   - Contextual word embeddings
   - Cross-modal attention (audio â†” text)
   - Multi-task learning (CTC + Word2Vec)

### Embedding Systems âœ…

#### Our Enhanced Embeddings:
1. **BPE Tokenization** (`preprocessing/bpe_tokenizer.py`)
   - Subword tokenization for Vietnamese
   - Better OOV handling
   - Trained on transcripts

2. **Contextual Embeddings** (`models/enhanced_asr.py`)
   - Transformer-based word embeddings
   - Context-aware representations
   - Used for cross-modal attention

3. **Cross-Modal Attention** (`models/enhanced_asr.py`)
   - Audio attends to text
   - Text attends to audio
   - Enables contextual biasing

4. **Word2Vec Auxiliary** (`models/enhanced_asr.py`)
   - Multi-task learning
   - Auxiliary Word2Vec task during training
   - Improves embeddings quality

#### AI2text Embeddings Patch:
1. **Semantic Word2Vec** (`nlp/word2vec_trainer.py`)
   - Trained on transcripts
   - Skip-gram architecture
   - Semantic similarity matching

2. **Phon2Vec** (`nlp/phon2vec_trainer.py`)
   - Trained on phonetic tokens (Telex+tone)
   - Sound-based similarity
   - Handles rare/OOV words

3. **Phonetic Processing** (`preprocessing/phonetic.py`)
   - Telex encoding
   - VnSoundex
   - Tone detection

4. **N-best Rescoring** (`decoding/rescoring.py`)
   - Semantic similarity scoring
   - Phonetic similarity scoring
   - Contextual biasing

### Training Pipeline âœ…

1. **Multi-task Learning**
   - CTC loss (main task)
   - Word2Vec auxiliary loss (optional)
   - Shared encoder

2. **Training Optimizations**
   - Mixed precision (AMP)
   - Gradient clipping
   - OneCycleLR scheduler

3. **Callbacks** (`training/callbacks.py`)
   - CheckpointCallback
   - EarlyStoppingCallback
   - LoggingCallback
   - MetricsCallback

### Evaluation & Inference âœ…

1. **Beam Search Decoding** (`decoding/beam_search.py`)
   - Multiple hypotheses exploration
   - Length penalty
   - Better than greedy decoding

2. **N-best Rescoring** (`decoding/rescoring.py`)
   - Combines acoustic + semantic + phonetic scores
   - Contextual biasing support
   - Handles rare words

3. **Contextual Biasing** (`decoding/rescoring.py`)
   - Bias towards specific words (names, domain terms)
   - Uses semantic and phonetic similarity
   - Dynamic OOV handling

## ğŸ”„ Complete Data Flow

### Training Flow:
```
1. Data Layer:
   Raw Audio â†’ Database â†’ Split Assignment

2. Preprocessing Layer:
   Audio â†’ Mel Spectrogram (80 dim)
   Text â†’ BPE/Character Tokens â†’ Token IDs

3. Model Layer:
   Features (80, time) â†’ Encoder â†’ Cross-Modal Attention â† Context Embeddings
                         â†“
                      CTC Decoder â†’ Logits
                      Word2Vec Projection (optional)

4. Training Layer:
   Logits â†’ CTC Loss
   Word2Vec â†’ Auxiliary Loss (optional)
   Optimizer â†’ AdamW
   Callbacks â†’ Checkpoint, Logging, Metrics

5. Evaluation Layer:
   Model â†’ Beam Search â†’ N-best â†’ Rescoring â†’ Best Hypothesis
```

### Inference Flow:
```
1. Audio File â†’ Preprocessing â†’ Mel Spectrogram

2. Model â†’ Beam Search Decoding â†’ N-best Hypotheses

3. Rescoring:
   - Acoustic scores (from model)
   - Semantic similarity (Word2Vec)
   - Phonetic similarity (Phon2Vec)
   - Contextual biasing (if context provided)

4. Best Hypothesis â†’ Text Output
```

## ğŸ¯ Usage Examples

### 1. Train Basic LSTM Model:
```python
from models.lstm_asr import LSTMASRModel

model = LSTMASRModel(
    input_dim=80,
    vocab_size=1000,
    hidden_size=256,
    num_lstm_layers=3
)
```

### 2. Train Enhanced Transformer:
```python
from models.enhanced_asr import EnhancedASRModel

model = EnhancedASRModel(
    input_dim=80,
    vocab_size=1000,
    use_contextual_embeddings=True,
    use_cross_modal_attention=True,
    use_word2vec_auxiliary=True
)
```

### 3. Train Embeddings:
```bash
# Train Word2Vec and Phon2Vec
python scripts/build_embeddings.py --db database/asr_training.db --config configs/embeddings.yaml
```

### 4. Use BPE Tokenization:
```python
from preprocessing.bpe_tokenizer import BPETokenizer

tokenizer = BPETokenizer()
tokenizer.train(texts, vocab_size=1000)
token_ids = tokenizer.encode("xin chÃ o")
```

### 5. Beam Search Decoding:
```python
from decoding.beam_search import BeamSearchDecoder

decoder = BeamSearchDecoder(
    vocab_size=1000,
    blank_token_id=2,
    beam_width=5
)
nbest = decoder.decode(logits, lengths)
```

### 6. N-best Rescoring:
```python
from decoding.rescoring import rescore_nbest
from gensim.models import KeyedVectors

# Load embeddings
semantic_kv = KeyedVectors.load("models/embeddings/word2vec.kv")
phon_kv = KeyedVectors.load("models/embeddings/phon2vec.kv")

# Rescore N-best
context = "Ä‘áº·t bÃ¡nh gato sinh nháº­t"
rescored = rescore_nbest(
    nbest,
    semantic_kv=semantic_kv,
    phon_kv=phon_kv,
    context_text=context,
    gamma=0.5,
    delta=0.3
)
```

### 7. Contextual Biasing:
```python
from decoding.rescoring import contextual_biasing

bias_words = ["nguyá»…n", "há»“ chÃ­ minh", "hÃ  ná»™i"]
biased = contextual_biasing(
    nbest,
    bias_list=bias_words,
    semantic_kv=semantic_kv,
    phon_kv=phon_kv,
    bias_weight=0.3
)
```

## ğŸ“ Complete File Structure

```
AI2text/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ asr_base.py           âœ… Base Transformer + CTC
â”‚   â”œâ”€â”€ lstm_asr.py            âœ… LSTM + CTC (NEW)
â”‚   â”œâ”€â”€ enhanced_asr.py        âœ… Enhanced with embeddings (NEW)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ audio_processing.py    âœ… Audio features
â”‚   â”œâ”€â”€ text_cleaning.py       âœ… Text normalization
â”‚   â”œâ”€â”€ bpe_tokenizer.py       âœ… BPE tokenization (NEW)
â”‚   â”œâ”€â”€ phonetic.py            âœ… Phonetic processing (NEW)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ nlp/
â”‚   â”œâ”€â”€ word2vec_trainer.py    âœ… Word2Vec training (NEW)
â”‚   â”œâ”€â”€ phon2vec_trainer.py    âœ… Phon2Vec training (NEW)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ decoding/
â”‚   â”œâ”€â”€ beam_search.py         âœ… Beam search (NEW)
â”‚   â”œâ”€â”€ rescoring.py           âœ… N-best rescoring (NEW)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dataset.py             âœ… Dataset
â”‚   â”œâ”€â”€ train.py               âœ… Training with callbacks
â”‚   â”œâ”€â”€ evaluate.py            âœ… Evaluation
â”‚   â”œâ”€â”€ callbacks.py           âœ… Callbacks
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py        âœ… Data preparation
â”‚   â”œâ”€â”€ build_embeddings.py    âœ… Embeddings training (NEW)
â”‚   â””â”€â”€ validate_data.py       âœ… Validation
â”‚
â””â”€â”€ configs/
    â”œâ”€â”€ default.yaml           âœ… Training config
    â”œâ”€â”€ db.yaml                âœ… Database config
    â””â”€â”€ embeddings.yaml        âœ… Embeddings config (NEW)
```

## âœ¨ Key Features

### 1. Multiple Model Architectures
- **LSTM**: Fast, lightweight, good for weak hardware
- **Transformer**: State-of-the-art, best accuracy
- **Enhanced**: With contextual embeddings and cross-modal attention

### 2. Multiple Tokenization Methods
- **Character-level**: Simple, handles all words
- **BPE**: Subword units, better for OOV words

### 3. Embeddings for Contextual Biasing
- **Semantic (Word2Vec)**: Meaning-based similarity
- **Phonetic (Phon2Vec)**: Sound-based similarity
- **Contextual**: Transformer-based, context-aware

### 4. Advanced Decoding
- **Greedy**: Fast, single best hypothesis
- **Beam Search**: Multiple hypotheses, better accuracy
- **N-best Rescoring**: Combines multiple signals

### 5. Contextual Biasing
- **Dynamic**: Based on context text
- **Static**: Based on bias word lists
- **Domain-specific**: Custom vocabularies

---

**Your system now has ALL features from the specification!** ğŸ‰

